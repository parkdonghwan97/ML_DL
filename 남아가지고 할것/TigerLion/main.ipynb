{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f37432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8567d4a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CollectLinks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cb84ba7defff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mCollectLinks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollect_links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSites\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'CollectLinks'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright 2018 YoongiKim\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "import argparse\n",
    "import imghdr\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import random\n",
    "from CollectLinks import collect_links\n",
    "\n",
    "class Sites:\n",
    "    GOOGLE = 1\n",
    "    NAVER = 2\n",
    "    GOOGLE_FULL = 3\n",
    "    NAVER_FULL = 4\n",
    "\n",
    "    @staticmethod\n",
    "    def get_text(code):\n",
    "        if code == Sites.GOOGLE:\n",
    "            return 'google'\n",
    "        elif code == Sites.NAVER:\n",
    "            return 'naver'\n",
    "        elif code == Sites.GOOGLE_FULL:\n",
    "            return 'google'\n",
    "        elif code == Sites.NAVER_FULL:\n",
    "            return 'naver'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_face_url(code):\n",
    "        if code == Sites.GOOGLE or Sites.GOOGLE_FULL:\n",
    "            return \"&tbs=itp:face\"\n",
    "        if code == Sites.NAVER or Sites.NAVER_FULL:\n",
    "            return \"&face=1\"\n",
    "\n",
    "\n",
    "class AutoCrawler:\n",
    "    def __init__(self, skip_already_exist=True, n_threads=4, do_google=True, do_naver=True, download_path='download',\n",
    "                 full_resolution=False, face=False, no_gui=False, limit=0, proxy_list=None):\n",
    "        \"\"\"\n",
    "        :param skip_already_exist: Skips keyword already downloaded before. This is needed when re-downloading.\n",
    "        :param n_threads: Number of threads to download.\n",
    "        :param do_google: Download from google.com (boolean)\n",
    "        :param do_naver: Download from naver.com (boolean)\n",
    "        :param download_path: Download folder path\n",
    "        :param full_resolution: Download full resolution image instead of thumbnails (slow)\n",
    "        :param face: Face search mode\n",
    "        :param no_gui: No GUI mode. Acceleration for full_resolution mode.\n",
    "        :param limit: Maximum count of images to download. (0: infinite)\n",
    "        :param proxy_list: The proxy list. Every thread will randomly choose one from the list.\n",
    "        \"\"\"\n",
    "\n",
    "        self.skip = skip_already_exist\n",
    "        self.n_threads = n_threads\n",
    "        self.do_google = do_google\n",
    "        self.do_naver = do_naver\n",
    "        self.download_path = download_path\n",
    "        self.full_resolution = full_resolution\n",
    "        self.face = face\n",
    "        self.no_gui = no_gui\n",
    "        self.limit = limit\n",
    "        self.proxy_list = proxy_list if proxy_list and len(proxy_list) > 0 else None\n",
    "\n",
    "        os.makedirs('./{}'.format(self.download_path), exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def all_dirs(path):\n",
    "        paths = []\n",
    "        for dir in os.listdir(path):\n",
    "            if os.path.isdir(path + '/' + dir):\n",
    "                paths.append(path + '/' + dir)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    @staticmethod\n",
    "    def all_files(path):\n",
    "        paths = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if os.path.isfile(path + '/' + file):\n",
    "                    paths.append(path + '/' + file)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    @staticmethod\n",
    "    def get_extension_from_link(link, default='jpg'):\n",
    "        splits = str(link).split('.')\n",
    "        if len(splits) == 0:\n",
    "            return default\n",
    "        ext = splits[-1].lower()\n",
    "        if ext == 'jpg' or ext == 'jpeg':\n",
    "            return 'jpg'\n",
    "        elif ext == 'gif':\n",
    "            return 'gif'\n",
    "        elif ext == 'png':\n",
    "            return 'png'\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_image(path):\n",
    "        ext = imghdr.what(path)\n",
    "        if ext == 'jpeg':\n",
    "            ext = 'jpg'\n",
    "        return ext  # returns None if not valid\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dir(dirname):\n",
    "        current_path = os.getcwd()\n",
    "        path = os.path.join(current_path, dirname)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_keywords(keywords_file='keywords.txt'):\n",
    "        # read search keywords from file\n",
    "        with open(keywords_file, 'r', encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "            lines = text.split('\\n')\n",
    "            lines = filter(lambda x: x != '' and x is not None, lines)\n",
    "            keywords = sorted(set(lines))\n",
    "\n",
    "        print('{} keywords found: {}'.format(len(keywords), keywords))\n",
    "\n",
    "        # re-save sorted keywords\n",
    "        with open(keywords_file, 'w+', encoding='utf-8') as f:\n",
    "            for keyword in keywords:\n",
    "                f.write('{}\\n'.format(keyword))\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    @staticmethod\n",
    "    def save_object_to_file(object, file_path, is_base64=False):\n",
    "        try:\n",
    "            with open('{}'.format(file_path), 'wb') as file:\n",
    "                if is_base64:\n",
    "                    file.write(object)\n",
    "                else:\n",
    "                    shutil.copyfileobj(object.raw, file)\n",
    "        except Exception as e:\n",
    "            print('Save failed - {}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def base64_to_object(src):\n",
    "        header, encoded = str(src).split(',', 1)\n",
    "        data = base64.decodebytes(bytes(encoded, encoding='utf-8'))\n",
    "        return data\n",
    "\n",
    "    def download_images(self, keyword, links, site_name, max_count=0):\n",
    "        self.make_dir('{}/{}'.format(self.download_path, keyword.replace('\"', '')))\n",
    "        total = len(links)\n",
    "        success_count = 0\n",
    "\n",
    "        if max_count == 0:\n",
    "            max_count = total\n",
    "\n",
    "        for index, link in enumerate(links):\n",
    "            if success_count >= max_count:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                print('Downloading {} from {}: {} / {}'.format(keyword, site_name, success_count + 1, max_count))\n",
    "\n",
    "                if str(link).startswith('data:image/jpeg;base64'):\n",
    "                    response = self.base64_to_object(link)\n",
    "                    ext = 'jpg'\n",
    "                    is_base64 = True\n",
    "                elif str(link).startswith('data:image/png;base64'):\n",
    "                    response = self.base64_to_object(link)\n",
    "                    ext = 'png'\n",
    "                    is_base64 = True\n",
    "                else:\n",
    "                    response = requests.get(link, stream=True)\n",
    "                    ext = self.get_extension_from_link(link)\n",
    "                    is_base64 = False\n",
    "\n",
    "                no_ext_path = '{}/{}/{}_{}'.format(self.download_path.replace('\"', ''), keyword, site_name,\n",
    "                                                   str(index).zfill(4))\n",
    "                path = no_ext_path + '.' + ext\n",
    "                self.save_object_to_file(response, path, is_base64=is_base64)\n",
    "\n",
    "                success_count += 1\n",
    "                del response\n",
    "\n",
    "                ext2 = self.validate_image(path)\n",
    "                if ext2 is None:\n",
    "                    print('Unreadable file - {}'.format(link))\n",
    "                    os.remove(path)\n",
    "                    success_count -= 1\n",
    "                else:\n",
    "                    if ext != ext2:\n",
    "                        path2 = no_ext_path + '.' + ext2\n",
    "                        os.rename(path, path2)\n",
    "                        print('Renamed extension {} -> {}'.format(ext, ext2))\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Download failed - ', e)\n",
    "                continue\n",
    "\n",
    "    def download_from_site(self, keyword, site_code):\n",
    "        site_name = Sites.get_text(site_code)\n",
    "        add_url = Sites.get_face_url(site_code) if self.face else \"\"\n",
    "\n",
    "        try:\n",
    "            proxy = None\n",
    "            if self.proxy_list:\n",
    "                proxy = random.choice(self.proxy_list)\n",
    "            collect = CollectLinks(no_gui=self.no_gui, proxy=proxy)  # initialize chrome driver\n",
    "        except Exception as e:\n",
    "            print('Error occurred while initializing chromedriver - {}'.format(e))\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print('Collecting links... {} from {}'.format(keyword, site_name))\n",
    "\n",
    "            if site_code == Sites.GOOGLE:\n",
    "                links = collect.google(keyword, add_url)\n",
    "\n",
    "            elif site_code == Sites.NAVER:\n",
    "                links = collect.naver(keyword, add_url)\n",
    "\n",
    "            elif site_code == Sites.GOOGLE_FULL:\n",
    "                links = collect.google_full(keyword, add_url)\n",
    "\n",
    "            elif site_code == Sites.NAVER_FULL:\n",
    "                links = collect.naver_full(keyword, add_url)\n",
    "\n",
    "            else:\n",
    "                print('Invalid Site Code')\n",
    "                links = []\n",
    "\n",
    "            print('Downloading images from collected links... {} from {}'.format(keyword, site_name))\n",
    "            self.download_images(keyword, links, site_name, max_count=self.limit)\n",
    "            Path('{}/{}/{}_done'.format(self.download_path, keyword.replace('\"', ''), site_name)).touch()\n",
    "\n",
    "            print('Done {} : {}'.format(site_name, keyword))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception {}:{} - {}'.format(site_name, keyword, e))\n",
    "\n",
    "    def download(self, args):\n",
    "        self.download_from_site(keyword=args[0], site_code=args[1])\n",
    "\n",
    "    def do_crawling(self):\n",
    "        keywords = self.get_keywords()\n",
    "\n",
    "        tasks = []\n",
    "\n",
    "        for keyword in keywords:\n",
    "            dir_name = '{}/{}'.format(self.download_path, keyword)\n",
    "            google_done = os.path.exists(os.path.join(os.getcwd(), dir_name, 'google_done'))\n",
    "            naver_done = os.path.exists(os.path.join(os.getcwd(), dir_name, 'naver_done'))\n",
    "            if google_done and naver_done and self.skip:\n",
    "                print('Skipping done task {}'.format(dir_name))\n",
    "                continue\n",
    "\n",
    "            if self.do_google and not google_done:\n",
    "                if self.full_resolution:\n",
    "                    tasks.append([keyword, Sites.GOOGLE_FULL])\n",
    "                else:\n",
    "                    tasks.append([keyword, Sites.GOOGLE])\n",
    "\n",
    "            if self.do_naver and not naver_done:\n",
    "                if self.full_resolution:\n",
    "                    tasks.append([keyword, Sites.NAVER_FULL])\n",
    "                else:\n",
    "                    tasks.append([keyword, Sites.NAVER])\n",
    "\n",
    "        pool = Pool(self.n_threads)\n",
    "        pool.map_async(self.download, tasks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print('Task ended. Pool join.')\n",
    "\n",
    "        self.imbalance_check()\n",
    "\n",
    "        print('End Program')\n",
    "\n",
    "    def imbalance_check(self):\n",
    "        print('Data imbalance checking...')\n",
    "\n",
    "        dict_num_files = {}\n",
    "\n",
    "        for dir in self.all_dirs(self.download_path):\n",
    "            n_files = len(self.all_files(dir))\n",
    "            dict_num_files[dir] = n_files\n",
    "\n",
    "        avg = 0\n",
    "        for dir, n_files in dict_num_files.items():\n",
    "            avg += n_files / len(dict_num_files)\n",
    "            print('dir: {}, file_count: {}'.format(dir, n_files))\n",
    "\n",
    "        dict_too_small = {}\n",
    "\n",
    "        for dir, n_files in dict_num_files.items():\n",
    "            if n_files < avg * 0.5:\n",
    "                dict_too_small[dir] = n_files\n",
    "\n",
    "        if len(dict_too_small) >= 1:\n",
    "            print('Data imbalance detected.')\n",
    "            print('Below keywords have smaller than 50% of average file count.')\n",
    "            print('I recommend you to remove these directories and re-download for that keyword.')\n",
    "            print('_________________________________')\n",
    "            print('Too small file count directories:')\n",
    "            for dir, n_files in dict_too_small.items():\n",
    "                print('dir: {}, file_count: {}'.format(dir, n_files))\n",
    "\n",
    "            print(\"Remove directories above? (y/n)\")\n",
    "            answer = input()\n",
    "\n",
    "            if answer == 'y':\n",
    "                # removing directories too small files\n",
    "                print(\"Removing too small file count directories...\")\n",
    "                for dir, n_files in dict_too_small.items():\n",
    "                    shutil.rmtree(dir)\n",
    "                    print('Removed {}'.format(dir))\n",
    "\n",
    "                print('Now re-run this program to re-download removed files. (with skip_already_exist=True)')\n",
    "        else:\n",
    "            print('Data imbalance not detected.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--skip', type=str, default='true',\n",
    "                        help='Skips keyword already downloaded before. This is needed when re-downloading.')\n",
    "    parser.add_argument('--threads', type=int, default=4, help='Number of threads to download.')\n",
    "    parser.add_argument('--google', type=str, default='true', help='Download from google.com (boolean)')\n",
    "    parser.add_argument('--naver', type=str, default='true', help='Download from naver.com (boolean)')\n",
    "    parser.add_argument('--full', type=str, default='false',\n",
    "                        help='Download full resolution image instead of thumbnails (slow)')\n",
    "    parser.add_argument('--face', type=str, default='false', help='Face search mode')\n",
    "    parser.add_argument('--no_gui', type=str, default='auto',\n",
    "                        help='No GUI mode. Acceleration for full_resolution mode. '\n",
    "                             'But unstable on thumbnail mode. '\n",
    "                             'Default: \"auto\" - false if full=false, true if full=true')\n",
    "    parser.add_argument('--limit', type=int, default=0,\n",
    "                        help='Maximum count of images to download per site. (0: infinite)')\n",
    "    parser.add_argument('--proxy-list', type=str, default='',\n",
    "                        help='The comma separated proxy list like: \"socks://127.0.0.1:1080,http://127.0.0.1:1081\". '\n",
    "                             'Every thread will randomly choose one from the list.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    _skip = False if str(args.skip).lower() == 'false' else True\n",
    "    _threads = args.threads\n",
    "    _google = False if str(args.google).lower() == 'false' else True\n",
    "    _naver = False if str(args.naver).lower() == 'false' else True\n",
    "    _full = False if str(args.full).lower() == 'false' else True\n",
    "    _face = False if str(args.face).lower() == 'false' else True\n",
    "    _limit = int(args.limit)\n",
    "    _proxy_list = args.proxy_list.split(',')\n",
    "\n",
    "    no_gui_input = str(args.no_gui).lower()\n",
    "    if no_gui_input == 'auto':\n",
    "        _no_gui = _full\n",
    "    elif no_gui_input == 'true':\n",
    "        _no_gui = True\n",
    "    else:\n",
    "        _no_gui = False\n",
    "\n",
    "    print(\n",
    "        'Options - skip:{}, threads:{}, google:{}, naver:{}, full_resolution:{}, face:{}, no_gui:{}, limit:{}, _proxy_list:{}'\n",
    "            .format(_skip, _threads, _google, _naver, _full, _face, _no_gui, _limit, _proxy_list))\n",
    "\n",
    "    crawler = AutoCrawler(skip_already_exist=_skip, n_threads=_threads,\n",
    "                          do_google=_google, do_naver=_naver, full_resolution=_full,\n",
    "                          face=_face, no_gui=_no_gui, limit=_limit, proxy_list=_proxy_list)\n",
    "    crawler.do_crawling()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
