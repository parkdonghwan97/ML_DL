{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b92f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790e3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array( [[0,0],[0,1],[1,0],[1,1]] )\n",
    "y_data = np.array( [[0],[1],[1],[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bc1858",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi = LogisticRegression( max_iter=1000, verbose=1)\n",
    "model_logi.fit( x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399644d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5e774d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4743665",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23694cfe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.score(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d518fe",
   "metadata": {},
   "source": [
    "### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384e35d6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69938567\n",
      "Iteration 2, loss = 0.69711555\n",
      "Iteration 3, loss = 0.69491125\n",
      "Iteration 4, loss = 0.69279517\n",
      "Iteration 5, loss = 0.69071796\n",
      "Iteration 6, loss = 0.68881746\n",
      "Iteration 7, loss = 0.68695245\n",
      "Iteration 8, loss = 0.68516808\n",
      "Iteration 9, loss = 0.68345292\n",
      "Iteration 10, loss = 0.68183357\n",
      "Iteration 11, loss = 0.68027709\n",
      "Iteration 12, loss = 0.67878793\n",
      "Iteration 13, loss = 0.67735091\n",
      "Iteration 14, loss = 0.67597176\n",
      "Iteration 15, loss = 0.67466008\n",
      "Iteration 16, loss = 0.67339737\n",
      "Iteration 17, loss = 0.67215659\n",
      "Iteration 18, loss = 0.67093405\n",
      "Iteration 19, loss = 0.66975747\n",
      "Iteration 20, loss = 0.66857767\n",
      "Iteration 21, loss = 0.66739480\n",
      "Iteration 22, loss = 0.66621138\n",
      "Iteration 23, loss = 0.66505532\n",
      "Iteration 24, loss = 0.66395067\n",
      "Iteration 25, loss = 0.66282336\n",
      "Iteration 26, loss = 0.66170419\n",
      "Iteration 27, loss = 0.66060542\n",
      "Iteration 28, loss = 0.65950231\n",
      "Iteration 29, loss = 0.65840347\n",
      "Iteration 30, loss = 0.65728961\n",
      "Iteration 31, loss = 0.65617166\n",
      "Iteration 32, loss = 0.65504694\n",
      "Iteration 33, loss = 0.65391366\n",
      "Iteration 34, loss = 0.65279351\n",
      "Iteration 35, loss = 0.65170632\n",
      "Iteration 36, loss = 0.65059230\n",
      "Iteration 37, loss = 0.64953019\n",
      "Iteration 38, loss = 0.64845736\n",
      "Iteration 39, loss = 0.64743982\n",
      "Iteration 40, loss = 0.64641721\n",
      "Iteration 41, loss = 0.64539765\n",
      "Iteration 42, loss = 0.64436817\n",
      "Iteration 43, loss = 0.64333095\n",
      "Iteration 44, loss = 0.64228612\n",
      "Iteration 45, loss = 0.64130432\n",
      "Iteration 46, loss = 0.64027782\n",
      "Iteration 47, loss = 0.63923430\n",
      "Iteration 48, loss = 0.63819152\n",
      "Iteration 49, loss = 0.63714853\n",
      "Iteration 50, loss = 0.63608744\n",
      "Iteration 51, loss = 0.63502969\n",
      "Iteration 52, loss = 0.63399017\n",
      "Iteration 53, loss = 0.63293824\n",
      "Iteration 54, loss = 0.63187051\n",
      "Iteration 55, loss = 0.63078574\n",
      "Iteration 56, loss = 0.62970939\n",
      "Iteration 57, loss = 0.62860304\n",
      "Iteration 58, loss = 0.62750085\n",
      "Iteration 59, loss = 0.62643595\n",
      "Iteration 60, loss = 0.62534666\n",
      "Iteration 61, loss = 0.62426081\n",
      "Iteration 62, loss = 0.62317062\n",
      "Iteration 63, loss = 0.62210396\n",
      "Iteration 64, loss = 0.62102544\n",
      "Iteration 65, loss = 0.61996032\n",
      "Iteration 66, loss = 0.61890564\n",
      "Iteration 67, loss = 0.61782845\n",
      "Iteration 68, loss = 0.61674644\n",
      "Iteration 69, loss = 0.61567624\n",
      "Iteration 70, loss = 0.61457911\n",
      "Iteration 71, loss = 0.61348035\n",
      "Iteration 72, loss = 0.61238413\n",
      "Iteration 73, loss = 0.61129178\n",
      "Iteration 74, loss = 0.61017269\n",
      "Iteration 75, loss = 0.60905236\n",
      "Iteration 76, loss = 0.60793651\n",
      "Iteration 77, loss = 0.60679013\n",
      "Iteration 78, loss = 0.60566566\n",
      "Iteration 79, loss = 0.60454250\n",
      "Iteration 80, loss = 0.60340947\n",
      "Iteration 81, loss = 0.60226770\n",
      "Iteration 82, loss = 0.60112213\n",
      "Iteration 83, loss = 0.59996222\n",
      "Iteration 84, loss = 0.59878280\n",
      "Iteration 85, loss = 0.59761974\n",
      "Iteration 86, loss = 0.59644410\n",
      "Iteration 87, loss = 0.59527631\n",
      "Iteration 88, loss = 0.59410026\n",
      "Iteration 89, loss = 0.59293034\n",
      "Iteration 90, loss = 0.59175704\n",
      "Iteration 91, loss = 0.59057448\n",
      "Iteration 92, loss = 0.58937671\n",
      "Iteration 93, loss = 0.58817938\n",
      "Iteration 94, loss = 0.58696533\n",
      "Iteration 95, loss = 0.58575712\n",
      "Iteration 96, loss = 0.58454938\n",
      "Iteration 97, loss = 0.58332063\n",
      "Iteration 98, loss = 0.58211463\n",
      "Iteration 99, loss = 0.58088079\n",
      "Iteration 100, loss = 0.57962648\n",
      "Iteration 101, loss = 0.57843320\n",
      "Iteration 102, loss = 0.57718103\n",
      "Iteration 103, loss = 0.57590260\n",
      "Iteration 104, loss = 0.57463154\n",
      "Iteration 105, loss = 0.57336506\n",
      "Iteration 106, loss = 0.57208359\n",
      "Iteration 107, loss = 0.57080521\n",
      "Iteration 108, loss = 0.56954327\n",
      "Iteration 109, loss = 0.56831188\n",
      "Iteration 110, loss = 0.56703050\n",
      "Iteration 111, loss = 0.56569254\n",
      "Iteration 112, loss = 0.56438449\n",
      "Iteration 113, loss = 0.56308910\n",
      "Iteration 114, loss = 0.56180737\n",
      "Iteration 115, loss = 0.56048172\n",
      "Iteration 116, loss = 0.55917776\n",
      "Iteration 117, loss = 0.55786904\n",
      "Iteration 118, loss = 0.55652233\n",
      "Iteration 119, loss = 0.55515300\n",
      "Iteration 120, loss = 0.55382293\n",
      "Iteration 121, loss = 0.55250229\n",
      "Iteration 122, loss = 0.55118233\n",
      "Iteration 123, loss = 0.54981431\n",
      "Iteration 124, loss = 0.54844134\n",
      "Iteration 125, loss = 0.54708002\n",
      "Iteration 126, loss = 0.54569524\n",
      "Iteration 127, loss = 0.54433865\n",
      "Iteration 128, loss = 0.54294292\n",
      "Iteration 129, loss = 0.54151730\n",
      "Iteration 130, loss = 0.54012445\n",
      "Iteration 131, loss = 0.53873863\n",
      "Iteration 132, loss = 0.53733362\n",
      "Iteration 133, loss = 0.53589795\n",
      "Iteration 134, loss = 0.53448997\n",
      "Iteration 135, loss = 0.53308334\n",
      "Iteration 136, loss = 0.53162346\n",
      "Iteration 137, loss = 0.53016066\n",
      "Iteration 138, loss = 0.52871975\n",
      "Iteration 139, loss = 0.52725969\n",
      "Iteration 140, loss = 0.52580488\n",
      "Iteration 141, loss = 0.52434946\n",
      "Iteration 142, loss = 0.52290559\n",
      "Iteration 143, loss = 0.52141012\n",
      "Iteration 144, loss = 0.51991016\n",
      "Iteration 145, loss = 0.51843233\n",
      "Iteration 146, loss = 0.51697458\n",
      "Iteration 147, loss = 0.51551173\n",
      "Iteration 148, loss = 0.51401483\n",
      "Iteration 149, loss = 0.51251044\n",
      "Iteration 150, loss = 0.51100861\n",
      "Iteration 151, loss = 0.50951915\n",
      "Iteration 152, loss = 0.50804511\n",
      "Iteration 153, loss = 0.50654816\n",
      "Iteration 154, loss = 0.50502959\n",
      "Iteration 155, loss = 0.50352398\n",
      "Iteration 156, loss = 0.50199716\n",
      "Iteration 157, loss = 0.50048230\n",
      "Iteration 158, loss = 0.49895124\n",
      "Iteration 159, loss = 0.49737845\n",
      "Iteration 160, loss = 0.49583728\n",
      "Iteration 161, loss = 0.49430097\n",
      "Iteration 162, loss = 0.49278177\n",
      "Iteration 163, loss = 0.49126652\n",
      "Iteration 164, loss = 0.48973777\n",
      "Iteration 165, loss = 0.48821833\n",
      "Iteration 166, loss = 0.48664885\n",
      "Iteration 167, loss = 0.48514336\n",
      "Iteration 168, loss = 0.48361142\n",
      "Iteration 169, loss = 0.48208081\n",
      "Iteration 170, loss = 0.48055152\n",
      "Iteration 171, loss = 0.47901829\n",
      "Iteration 172, loss = 0.47746182\n",
      "Iteration 173, loss = 0.47586581\n",
      "Iteration 174, loss = 0.47429946\n",
      "Iteration 175, loss = 0.47274216\n",
      "Iteration 176, loss = 0.47118466\n",
      "Iteration 177, loss = 0.46965203\n",
      "Iteration 178, loss = 0.46810626\n",
      "Iteration 179, loss = 0.46654454\n",
      "Iteration 180, loss = 0.46498682\n",
      "Iteration 181, loss = 0.46341653\n",
      "Iteration 182, loss = 0.46187699\n",
      "Iteration 183, loss = 0.46030836\n",
      "Iteration 184, loss = 0.45874786\n",
      "Iteration 185, loss = 0.45718254\n",
      "Iteration 186, loss = 0.45565279\n",
      "Iteration 187, loss = 0.45409242\n",
      "Iteration 188, loss = 0.45248496\n",
      "Iteration 189, loss = 0.45091387\n",
      "Iteration 190, loss = 0.44939974\n",
      "Iteration 191, loss = 0.44783408\n",
      "Iteration 192, loss = 0.44625063\n",
      "Iteration 193, loss = 0.44472096\n",
      "Iteration 194, loss = 0.44314133\n",
      "Iteration 195, loss = 0.44160103\n",
      "Iteration 196, loss = 0.44003507\n",
      "Iteration 197, loss = 0.43847252\n",
      "Iteration 198, loss = 0.43688223\n",
      "Iteration 199, loss = 0.43530417\n",
      "Iteration 200, loss = 0.43377475\n",
      "Iteration 201, loss = 0.43220198\n",
      "Iteration 202, loss = 0.43064539\n",
      "Iteration 203, loss = 0.42908345\n",
      "Iteration 204, loss = 0.42749440\n",
      "Iteration 205, loss = 0.42591183\n",
      "Iteration 206, loss = 0.42429790\n",
      "Iteration 207, loss = 0.42270645\n",
      "Iteration 208, loss = 0.42114274\n",
      "Iteration 209, loss = 0.41960178\n",
      "Iteration 210, loss = 0.41805132\n",
      "Iteration 211, loss = 0.41648111\n",
      "Iteration 212, loss = 0.41488048\n",
      "Iteration 213, loss = 0.41329244\n",
      "Iteration 214, loss = 0.41175203\n",
      "Iteration 215, loss = 0.41016825\n",
      "Iteration 216, loss = 0.40858255\n",
      "Iteration 217, loss = 0.40698645\n",
      "Iteration 218, loss = 0.40541676\n",
      "Iteration 219, loss = 0.40380725\n",
      "Iteration 220, loss = 0.40224755\n",
      "Iteration 221, loss = 0.40063059\n",
      "Iteration 222, loss = 0.39905956\n",
      "Iteration 223, loss = 0.39754005\n",
      "Iteration 224, loss = 0.39592210\n",
      "Iteration 225, loss = 0.39436592\n",
      "Iteration 226, loss = 0.39284265\n",
      "Iteration 227, loss = 0.39127746\n",
      "Iteration 228, loss = 0.38966643\n",
      "Iteration 229, loss = 0.38811942\n",
      "Iteration 230, loss = 0.38655640\n",
      "Iteration 231, loss = 0.38497019\n",
      "Iteration 232, loss = 0.38344111\n",
      "Iteration 233, loss = 0.38187009\n",
      "Iteration 234, loss = 0.38033628\n",
      "Iteration 235, loss = 0.37876212\n",
      "Iteration 236, loss = 0.37719036\n",
      "Iteration 237, loss = 0.37564091\n",
      "Iteration 238, loss = 0.37409564\n",
      "Iteration 239, loss = 0.37251635\n",
      "Iteration 240, loss = 0.37096251\n",
      "Iteration 241, loss = 0.36944232\n",
      "Iteration 242, loss = 0.36790225\n",
      "Iteration 243, loss = 0.36635093\n",
      "Iteration 244, loss = 0.36481790\n",
      "Iteration 245, loss = 0.36328219\n",
      "Iteration 246, loss = 0.36173890\n",
      "Iteration 247, loss = 0.36022860\n",
      "Iteration 248, loss = 0.35869387\n",
      "Iteration 249, loss = 0.35717402\n",
      "Iteration 250, loss = 0.35564728\n",
      "Iteration 251, loss = 0.35411806\n",
      "Iteration 252, loss = 0.35260442\n",
      "Iteration 253, loss = 0.35110739\n",
      "Iteration 254, loss = 0.34959252\n",
      "Iteration 255, loss = 0.34809655\n",
      "Iteration 256, loss = 0.34655963\n",
      "Iteration 257, loss = 0.34507884\n",
      "Iteration 258, loss = 0.34359474\n",
      "Iteration 259, loss = 0.34206809\n",
      "Iteration 260, loss = 0.34053496\n",
      "Iteration 261, loss = 0.33903159\n",
      "Iteration 262, loss = 0.33760377\n",
      "Iteration 263, loss = 0.33613370\n",
      "Iteration 264, loss = 0.33462719\n",
      "Iteration 265, loss = 0.33312606\n",
      "Iteration 266, loss = 0.33166282\n",
      "Iteration 267, loss = 0.33021803\n",
      "Iteration 268, loss = 0.32876335\n",
      "Iteration 269, loss = 0.32725216\n",
      "Iteration 270, loss = 0.32578905\n",
      "Iteration 271, loss = 0.32434174\n",
      "Iteration 272, loss = 0.32288963\n",
      "Iteration 273, loss = 0.32143534\n",
      "Iteration 274, loss = 0.31999628\n",
      "Iteration 275, loss = 0.31856581\n",
      "Iteration 276, loss = 0.31711037\n",
      "Iteration 277, loss = 0.31567165\n",
      "Iteration 278, loss = 0.31423553\n",
      "Iteration 279, loss = 0.31281597\n",
      "Iteration 280, loss = 0.31140636\n",
      "Iteration 281, loss = 0.30995715\n",
      "Iteration 282, loss = 0.30859561\n",
      "Iteration 283, loss = 0.30719218\n",
      "Iteration 284, loss = 0.30577101\n",
      "Iteration 285, loss = 0.30437048\n",
      "Iteration 286, loss = 0.30297462\n",
      "Iteration 287, loss = 0.30157570\n",
      "Iteration 288, loss = 0.30018417\n",
      "Iteration 289, loss = 0.29877329\n",
      "Iteration 290, loss = 0.29743667\n",
      "Iteration 291, loss = 0.29606706\n",
      "Iteration 292, loss = 0.29469551\n",
      "Iteration 293, loss = 0.29335723\n",
      "Iteration 294, loss = 0.29199146\n",
      "Iteration 295, loss = 0.29065714\n",
      "Iteration 296, loss = 0.28931156\n",
      "Iteration 297, loss = 0.28794008\n",
      "Iteration 298, loss = 0.28664024\n",
      "Iteration 299, loss = 0.28532008\n",
      "Iteration 300, loss = 0.28398335\n",
      "Iteration 301, loss = 0.28268163\n",
      "Iteration 302, loss = 0.28136104\n",
      "Iteration 303, loss = 0.28003332\n",
      "Iteration 304, loss = 0.27876838\n",
      "Iteration 305, loss = 0.27745557\n",
      "Iteration 306, loss = 0.27615371\n",
      "Iteration 307, loss = 0.27486163\n",
      "Iteration 308, loss = 0.27360116\n",
      "Iteration 309, loss = 0.27230904\n",
      "Iteration 310, loss = 0.27104133\n",
      "Iteration 311, loss = 0.26974696\n",
      "Iteration 312, loss = 0.26850236\n",
      "Iteration 313, loss = 0.26727036\n",
      "Iteration 314, loss = 0.26599065\n",
      "Iteration 315, loss = 0.26473441\n",
      "Iteration 316, loss = 0.26350237\n",
      "Iteration 317, loss = 0.26225555\n",
      "Iteration 318, loss = 0.26098915\n",
      "Iteration 319, loss = 0.25977559\n",
      "Iteration 320, loss = 0.25855952\n",
      "Iteration 321, loss = 0.25731595\n",
      "Iteration 322, loss = 0.25612081\n",
      "Iteration 323, loss = 0.25492539\n",
      "Iteration 324, loss = 0.25372028\n",
      "Iteration 325, loss = 0.25251799\n",
      "Iteration 326, loss = 0.25131099\n",
      "Iteration 327, loss = 0.25011441\n",
      "Iteration 328, loss = 0.24893770\n",
      "Iteration 329, loss = 0.24777303\n",
      "Iteration 330, loss = 0.24657138\n",
      "Iteration 331, loss = 0.24537623\n",
      "Iteration 332, loss = 0.24420619\n",
      "Iteration 333, loss = 0.24304953\n",
      "Iteration 334, loss = 0.24188208\n",
      "Iteration 335, loss = 0.24072541\n",
      "Iteration 336, loss = 0.23959581\n",
      "Iteration 337, loss = 0.23842941\n",
      "Iteration 338, loss = 0.23727171\n",
      "Iteration 339, loss = 0.23615872\n",
      "Iteration 340, loss = 0.23503325\n",
      "Iteration 341, loss = 0.23389766\n",
      "Iteration 342, loss = 0.23277566\n",
      "Iteration 343, loss = 0.23167018\n",
      "Iteration 344, loss = 0.23056255\n",
      "Iteration 345, loss = 0.22946027\n",
      "Iteration 346, loss = 0.22834835\n",
      "Iteration 347, loss = 0.22724188\n",
      "Iteration 348, loss = 0.22615420\n",
      "Iteration 349, loss = 0.22507726\n",
      "Iteration 350, loss = 0.22402265\n",
      "Iteration 351, loss = 0.22293241\n",
      "Iteration 352, loss = 0.22186969\n",
      "Iteration 353, loss = 0.22081592\n",
      "Iteration 354, loss = 0.21976398\n",
      "Iteration 355, loss = 0.21870168\n",
      "Iteration 356, loss = 0.21765996\n",
      "Iteration 357, loss = 0.21660242\n",
      "Iteration 358, loss = 0.21554602\n",
      "Iteration 359, loss = 0.21450938\n",
      "Iteration 360, loss = 0.21349821\n",
      "Iteration 361, loss = 0.21247921\n",
      "Iteration 362, loss = 0.21145485\n",
      "Iteration 363, loss = 0.21043323\n",
      "Iteration 364, loss = 0.20941191\n",
      "Iteration 365, loss = 0.20840486\n",
      "Iteration 366, loss = 0.20741288\n",
      "Iteration 367, loss = 0.20639382\n",
      "Iteration 368, loss = 0.20540339\n",
      "Iteration 369, loss = 0.20440445\n",
      "Iteration 370, loss = 0.20343128\n",
      "Iteration 371, loss = 0.20246136\n",
      "Iteration 372, loss = 0.20149758\n",
      "Iteration 373, loss = 0.20052828\n",
      "Iteration 374, loss = 0.19956244\n",
      "Iteration 375, loss = 0.19860001\n",
      "Iteration 376, loss = 0.19765975\n",
      "Iteration 377, loss = 0.19670388\n",
      "Iteration 378, loss = 0.19577093\n",
      "Iteration 379, loss = 0.19482115\n",
      "Iteration 380, loss = 0.19391679\n",
      "Iteration 381, loss = 0.19297038\n",
      "Iteration 382, loss = 0.19203610\n",
      "Iteration 383, loss = 0.19110749\n",
      "Iteration 384, loss = 0.19021231\n",
      "Iteration 385, loss = 0.18929485\n",
      "Iteration 386, loss = 0.18838811\n",
      "Iteration 387, loss = 0.18747779\n",
      "Iteration 388, loss = 0.18659238\n",
      "Iteration 389, loss = 0.18567761\n",
      "Iteration 390, loss = 0.18478331\n",
      "Iteration 391, loss = 0.18390523\n",
      "Iteration 392, loss = 0.18303810\n",
      "Iteration 393, loss = 0.18217208\n",
      "Iteration 394, loss = 0.18128628\n",
      "Iteration 395, loss = 0.18041545\n",
      "Iteration 396, loss = 0.17954803\n",
      "Iteration 397, loss = 0.17868049\n",
      "Iteration 398, loss = 0.17783074\n",
      "Iteration 399, loss = 0.17698159\n",
      "Iteration 400, loss = 0.17613902\n",
      "Iteration 401, loss = 0.17529750\n",
      "Iteration 402, loss = 0.17443486\n",
      "Iteration 403, loss = 0.17361375\n",
      "Iteration 404, loss = 0.17280177\n",
      "Iteration 405, loss = 0.17197120\n",
      "Iteration 406, loss = 0.17115711\n",
      "Iteration 407, loss = 0.17034404\n",
      "Iteration 408, loss = 0.16952240\n",
      "Iteration 409, loss = 0.16870594\n",
      "Iteration 410, loss = 0.16790246\n",
      "Iteration 411, loss = 0.16711563\n",
      "Iteration 412, loss = 0.16631049\n",
      "Iteration 413, loss = 0.16552347\n",
      "Iteration 414, loss = 0.16475452\n",
      "Iteration 415, loss = 0.16395972\n",
      "Iteration 416, loss = 0.16319315\n",
      "Iteration 417, loss = 0.16241903\n",
      "Iteration 418, loss = 0.16165215\n",
      "Iteration 419, loss = 0.16088634\n",
      "Iteration 420, loss = 0.16012258\n",
      "Iteration 421, loss = 0.15937251\n",
      "Iteration 422, loss = 0.15861196\n",
      "Iteration 423, loss = 0.15787398\n",
      "Iteration 424, loss = 0.15713308\n",
      "Iteration 425, loss = 0.15638052\n",
      "Iteration 426, loss = 0.15564509\n",
      "Iteration 427, loss = 0.15491295\n",
      "Iteration 428, loss = 0.15417571\n",
      "Iteration 429, loss = 0.15345926\n",
      "Iteration 430, loss = 0.15273577\n",
      "Iteration 431, loss = 0.15201786\n",
      "Iteration 432, loss = 0.15128981\n",
      "Iteration 433, loss = 0.15058139\n",
      "Iteration 434, loss = 0.14988687\n",
      "Iteration 435, loss = 0.14919293\n",
      "Iteration 436, loss = 0.14848830\n",
      "Iteration 437, loss = 0.14779051\n",
      "Iteration 438, loss = 0.14709301\n",
      "Iteration 439, loss = 0.14642251\n",
      "Iteration 440, loss = 0.14574545\n",
      "Iteration 441, loss = 0.14505403\n",
      "Iteration 442, loss = 0.14437727\n",
      "Iteration 443, loss = 0.14370858\n",
      "Iteration 444, loss = 0.14302726\n",
      "Iteration 445, loss = 0.14237494\n",
      "Iteration 446, loss = 0.14170691\n",
      "Iteration 447, loss = 0.14105097\n",
      "Iteration 448, loss = 0.14040166\n",
      "Iteration 449, loss = 0.13975729\n",
      "Iteration 450, loss = 0.13911018\n",
      "Iteration 451, loss = 0.13846456\n",
      "Iteration 452, loss = 0.13781881\n",
      "Iteration 453, loss = 0.13718674\n",
      "Iteration 454, loss = 0.13656065\n",
      "Iteration 455, loss = 0.13594051\n",
      "Iteration 456, loss = 0.13532930\n",
      "Iteration 457, loss = 0.13469823\n",
      "Iteration 458, loss = 0.13408277\n",
      "Iteration 459, loss = 0.13347471\n",
      "Iteration 460, loss = 0.13286010\n",
      "Iteration 461, loss = 0.13224257\n",
      "Iteration 462, loss = 0.13164516\n",
      "Iteration 463, loss = 0.13104738\n",
      "Iteration 464, loss = 0.13046780\n",
      "Iteration 465, loss = 0.12986691\n",
      "Iteration 466, loss = 0.12927842\n",
      "Iteration 467, loss = 0.12870462\n",
      "Iteration 468, loss = 0.12812642\n",
      "Iteration 469, loss = 0.12753761\n",
      "Iteration 470, loss = 0.12696942\n",
      "Iteration 471, loss = 0.12639209\n",
      "Iteration 472, loss = 0.12581409\n",
      "Iteration 473, loss = 0.12524938\n",
      "Iteration 474, loss = 0.12467740\n",
      "Iteration 475, loss = 0.12412133\n",
      "Iteration 476, loss = 0.12357284\n",
      "Iteration 477, loss = 0.12301478\n",
      "Iteration 478, loss = 0.12246696\n",
      "Iteration 479, loss = 0.12190489\n",
      "Iteration 480, loss = 0.12135570\n",
      "Iteration 481, loss = 0.12081019\n",
      "Iteration 482, loss = 0.12026125\n",
      "Iteration 483, loss = 0.11973787\n",
      "Iteration 484, loss = 0.11919996\n",
      "Iteration 485, loss = 0.11865310\n",
      "Iteration 486, loss = 0.11811405\n",
      "Iteration 487, loss = 0.11757568\n",
      "Iteration 488, loss = 0.11701619\n",
      "Iteration 489, loss = 0.11646571\n",
      "Iteration 490, loss = 0.11592221\n",
      "Iteration 491, loss = 0.11537299\n",
      "Iteration 492, loss = 0.11482242\n",
      "Iteration 493, loss = 0.11427142\n",
      "Iteration 494, loss = 0.11373914\n",
      "Iteration 495, loss = 0.11319485\n",
      "Iteration 496, loss = 0.11264502\n",
      "Iteration 497, loss = 0.11210782\n",
      "Iteration 498, loss = 0.11157293\n",
      "Iteration 499, loss = 0.11104676\n",
      "Iteration 500, loss = 0.11051476\n",
      "Iteration 501, loss = 0.10998413\n",
      "Iteration 502, loss = 0.10945884\n",
      "Iteration 503, loss = 0.10893835\n",
      "Iteration 504, loss = 0.10841841\n",
      "Iteration 505, loss = 0.10790147\n",
      "Iteration 506, loss = 0.10737730\n",
      "Iteration 507, loss = 0.10687083\n",
      "Iteration 508, loss = 0.10639377\n",
      "Iteration 509, loss = 0.10592436\n",
      "Iteration 510, loss = 0.10544420\n",
      "Iteration 511, loss = 0.10498605\n",
      "Iteration 512, loss = 0.10451349\n",
      "Iteration 513, loss = 0.10404218\n",
      "Iteration 514, loss = 0.10357424\n",
      "Iteration 515, loss = 0.10310773\n",
      "Iteration 516, loss = 0.10264346\n",
      "Iteration 517, loss = 0.10218001\n",
      "Iteration 518, loss = 0.10172363\n",
      "Iteration 519, loss = 0.10125581\n",
      "Iteration 520, loss = 0.10078431\n",
      "Iteration 521, loss = 0.10032156\n",
      "Iteration 522, loss = 0.09987135\n",
      "Iteration 523, loss = 0.09942114\n",
      "Iteration 524, loss = 0.09896987\n",
      "Iteration 525, loss = 0.09851582\n",
      "Iteration 526, loss = 0.09805963\n",
      "Iteration 527, loss = 0.09761279\n",
      "Iteration 528, loss = 0.09717438\n",
      "Iteration 529, loss = 0.09673258\n",
      "Iteration 530, loss = 0.09629797\n",
      "Iteration 531, loss = 0.09587114\n",
      "Iteration 532, loss = 0.09545463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 533, loss = 0.09502770\n",
      "Iteration 534, loss = 0.09459670\n",
      "Iteration 535, loss = 0.09416858\n",
      "Iteration 536, loss = 0.09374063\n",
      "Iteration 537, loss = 0.09332152\n",
      "Iteration 538, loss = 0.09290576\n",
      "Iteration 539, loss = 0.09249749\n",
      "Iteration 540, loss = 0.09209135\n",
      "Iteration 541, loss = 0.09168905\n",
      "Iteration 542, loss = 0.09128563\n",
      "Iteration 543, loss = 0.09087185\n",
      "Iteration 544, loss = 0.09047198\n",
      "Iteration 545, loss = 0.09007372\n",
      "Iteration 546, loss = 0.08967536\n",
      "Iteration 547, loss = 0.08928745\n",
      "Iteration 548, loss = 0.08889216\n",
      "Iteration 549, loss = 0.08851427\n",
      "Iteration 550, loss = 0.08813152\n",
      "Iteration 551, loss = 0.08774658\n",
      "Iteration 552, loss = 0.08735687\n",
      "Iteration 553, loss = 0.08696591\n",
      "Iteration 554, loss = 0.08659707\n",
      "Iteration 555, loss = 0.08622525\n",
      "Iteration 556, loss = 0.08583965\n",
      "Iteration 557, loss = 0.08547189\n",
      "Iteration 558, loss = 0.08510900\n",
      "Iteration 559, loss = 0.08474293\n",
      "Iteration 560, loss = 0.08437598\n",
      "Iteration 561, loss = 0.08400929\n",
      "Iteration 562, loss = 0.08365039\n",
      "Iteration 563, loss = 0.08328928\n",
      "Iteration 564, loss = 0.08293243\n",
      "Iteration 565, loss = 0.08257274\n",
      "Iteration 566, loss = 0.08221772\n",
      "Iteration 567, loss = 0.08187337\n",
      "Iteration 568, loss = 0.08152089\n",
      "Iteration 569, loss = 0.08116773\n",
      "Iteration 570, loss = 0.08082495\n",
      "Iteration 571, loss = 0.08048141\n",
      "Iteration 572, loss = 0.08013970\n",
      "Iteration 573, loss = 0.07979256\n",
      "Iteration 574, loss = 0.07945125\n",
      "Iteration 575, loss = 0.07911241\n",
      "Iteration 576, loss = 0.07878081\n",
      "Iteration 577, loss = 0.07845386\n",
      "Iteration 578, loss = 0.07812667\n",
      "Iteration 579, loss = 0.07779007\n",
      "Iteration 580, loss = 0.07746780\n",
      "Iteration 581, loss = 0.07714559\n",
      "Iteration 582, loss = 0.07682195\n",
      "Iteration 583, loss = 0.07649242\n",
      "Iteration 584, loss = 0.07616606\n",
      "Iteration 585, loss = 0.07585123\n",
      "Iteration 586, loss = 0.07552501\n",
      "Iteration 587, loss = 0.07521788\n",
      "Iteration 588, loss = 0.07491010\n",
      "Iteration 589, loss = 0.07459400\n",
      "Iteration 590, loss = 0.07428605\n",
      "Iteration 591, loss = 0.07397253\n",
      "Iteration 592, loss = 0.07365915\n",
      "Iteration 593, loss = 0.07336117\n",
      "Iteration 594, loss = 0.07306179\n",
      "Iteration 595, loss = 0.07276020\n",
      "Iteration 596, loss = 0.07245851\n",
      "Iteration 597, loss = 0.07216648\n",
      "Iteration 598, loss = 0.07186732\n",
      "Iteration 599, loss = 0.07156554\n",
      "Iteration 600, loss = 0.07127629\n",
      "Iteration 601, loss = 0.07098903\n",
      "Iteration 602, loss = 0.07070464\n",
      "Iteration 603, loss = 0.07040793\n",
      "Iteration 604, loss = 0.07011182\n",
      "Iteration 605, loss = 0.06982988\n",
      "Iteration 606, loss = 0.06954996\n",
      "Iteration 607, loss = 0.06927135\n",
      "Iteration 608, loss = 0.06898611\n",
      "Iteration 609, loss = 0.06870183\n",
      "Iteration 610, loss = 0.06842493\n",
      "Iteration 611, loss = 0.06815664\n",
      "Iteration 612, loss = 0.06787305\n",
      "Iteration 613, loss = 0.06760375\n",
      "Iteration 614, loss = 0.06733460\n",
      "Iteration 615, loss = 0.06706192\n",
      "Iteration 616, loss = 0.06678858\n",
      "Iteration 617, loss = 0.06653333\n",
      "Iteration 618, loss = 0.06626186\n",
      "Iteration 619, loss = 0.06599129\n",
      "Iteration 620, loss = 0.06573465\n",
      "Iteration 621, loss = 0.06547115\n",
      "Iteration 622, loss = 0.06520585\n",
      "Iteration 623, loss = 0.06494360\n",
      "Iteration 624, loss = 0.06468654\n",
      "Iteration 625, loss = 0.06442978\n",
      "Iteration 626, loss = 0.06417421\n",
      "Iteration 627, loss = 0.06392183\n",
      "Iteration 628, loss = 0.06366675\n",
      "Iteration 629, loss = 0.06342357\n",
      "Iteration 630, loss = 0.06317346\n",
      "Iteration 631, loss = 0.06292204\n",
      "Iteration 632, loss = 0.06267078\n",
      "Iteration 633, loss = 0.06242765\n",
      "Iteration 634, loss = 0.06218781\n",
      "Iteration 635, loss = 0.06194122\n",
      "Iteration 636, loss = 0.06169952\n",
      "Iteration 637, loss = 0.06146173\n",
      "Iteration 638, loss = 0.06121929\n",
      "Iteration 639, loss = 0.06098090\n",
      "Iteration 640, loss = 0.06074563\n",
      "Iteration 641, loss = 0.06051154\n",
      "Iteration 642, loss = 0.06027647\n",
      "Iteration 643, loss = 0.06004112\n",
      "Iteration 644, loss = 0.05980869\n",
      "Iteration 645, loss = 0.05958196\n",
      "Iteration 646, loss = 0.05935014\n",
      "Iteration 647, loss = 0.05912236\n",
      "Iteration 648, loss = 0.05889879\n",
      "Iteration 649, loss = 0.05867403\n",
      "Iteration 650, loss = 0.05844854\n",
      "Iteration 651, loss = 0.05822353\n",
      "Iteration 652, loss = 0.05800274\n",
      "Iteration 653, loss = 0.05778142\n",
      "Iteration 654, loss = 0.05755808\n",
      "Iteration 655, loss = 0.05733850\n",
      "Iteration 656, loss = 0.05712258\n",
      "Iteration 657, loss = 0.05690907\n",
      "Iteration 658, loss = 0.05669261\n",
      "Iteration 659, loss = 0.05646906\n",
      "Iteration 660, loss = 0.05625383\n",
      "Iteration 661, loss = 0.05605069\n",
      "Iteration 662, loss = 0.05584050\n",
      "Iteration 663, loss = 0.05562651\n",
      "Iteration 664, loss = 0.05541404\n",
      "Iteration 665, loss = 0.05521291\n",
      "Iteration 666, loss = 0.05501107\n",
      "Iteration 667, loss = 0.05480161\n",
      "Iteration 668, loss = 0.05459456\n",
      "Iteration 669, loss = 0.05439061\n",
      "Iteration 670, loss = 0.05418374\n",
      "Iteration 671, loss = 0.05398882\n",
      "Iteration 672, loss = 0.05378914\n",
      "Iteration 673, loss = 0.05359115\n",
      "Iteration 674, loss = 0.05339388\n",
      "Iteration 675, loss = 0.05319430\n",
      "Iteration 676, loss = 0.05300135\n",
      "Iteration 677, loss = 0.05280753\n",
      "Iteration 678, loss = 0.05261474\n",
      "Iteration 679, loss = 0.05241514\n",
      "Iteration 680, loss = 0.05221795\n",
      "Iteration 681, loss = 0.05203116\n",
      "Iteration 682, loss = 0.05184213\n",
      "Iteration 683, loss = 0.05165599\n",
      "Iteration 684, loss = 0.05146723\n",
      "Iteration 685, loss = 0.05127615\n",
      "Iteration 686, loss = 0.05108745\n",
      "Iteration 687, loss = 0.05090141\n",
      "Iteration 688, loss = 0.05071374\n",
      "Iteration 689, loss = 0.05053236\n",
      "Iteration 690, loss = 0.05035016\n",
      "Iteration 691, loss = 0.05016981\n",
      "Iteration 692, loss = 0.04998754\n",
      "Iteration 693, loss = 0.04980655\n",
      "Iteration 694, loss = 0.04963249\n",
      "Iteration 695, loss = 0.04945299\n",
      "Iteration 696, loss = 0.04927119\n",
      "Iteration 697, loss = 0.04909800\n",
      "Iteration 698, loss = 0.04892548\n",
      "Iteration 699, loss = 0.04875286\n",
      "Iteration 700, loss = 0.04857711\n",
      "Iteration 701, loss = 0.04839988\n",
      "Iteration 702, loss = 0.04822868\n",
      "Iteration 703, loss = 0.04806064\n",
      "Iteration 704, loss = 0.04788939\n",
      "Iteration 705, loss = 0.04771679\n",
      "Iteration 706, loss = 0.04754906\n",
      "Iteration 707, loss = 0.04738005\n",
      "Iteration 708, loss = 0.04721652\n",
      "Iteration 709, loss = 0.04704902\n",
      "Iteration 710, loss = 0.04688139\n",
      "Iteration 711, loss = 0.04671544\n",
      "Iteration 712, loss = 0.04655251\n",
      "Iteration 713, loss = 0.04639105\n",
      "Iteration 714, loss = 0.04622864\n",
      "Iteration 715, loss = 0.04606538\n",
      "Iteration 716, loss = 0.04590747\n",
      "Iteration 717, loss = 0.04574790\n",
      "Iteration 718, loss = 0.04558820\n",
      "Iteration 719, loss = 0.04542891\n",
      "Iteration 720, loss = 0.04526990\n",
      "Iteration 721, loss = 0.04511603\n",
      "Iteration 722, loss = 0.04495771\n",
      "Iteration 723, loss = 0.04480445\n",
      "Iteration 724, loss = 0.04464806\n",
      "Iteration 725, loss = 0.04449093\n",
      "Iteration 726, loss = 0.04433507\n",
      "Iteration 727, loss = 0.04418621\n",
      "Iteration 728, loss = 0.04403691\n",
      "Iteration 729, loss = 0.04388553\n",
      "Iteration 730, loss = 0.04373421\n",
      "Iteration 731, loss = 0.04358572\n",
      "Iteration 732, loss = 0.04343909\n",
      "Iteration 733, loss = 0.04329025\n",
      "Iteration 734, loss = 0.04314249\n",
      "Iteration 735, loss = 0.04299713\n",
      "Iteration 736, loss = 0.04285345\n",
      "Iteration 737, loss = 0.04270824\n",
      "Iteration 738, loss = 0.04256305\n",
      "Iteration 739, loss = 0.04242246\n",
      "Iteration 740, loss = 0.04228047\n",
      "Iteration 741, loss = 0.04213651\n",
      "Iteration 742, loss = 0.04199264\n",
      "Iteration 743, loss = 0.04185405\n",
      "Iteration 744, loss = 0.04171693\n",
      "Iteration 745, loss = 0.04157612\n",
      "Iteration 746, loss = 0.04143537\n",
      "Iteration 747, loss = 0.04129675\n",
      "Iteration 748, loss = 0.04115912\n",
      "Iteration 749, loss = 0.04102598\n",
      "Iteration 750, loss = 0.04088908\n",
      "Iteration 751, loss = 0.04075126\n",
      "Iteration 752, loss = 0.04061583\n",
      "Iteration 753, loss = 0.04048085\n",
      "Iteration 754, loss = 0.04034826\n",
      "Iteration 755, loss = 0.04021980\n",
      "Iteration 756, loss = 0.04008646\n",
      "Iteration 757, loss = 0.03994937\n",
      "Iteration 758, loss = 0.03981789\n",
      "Iteration 759, loss = 0.03968936\n",
      "Iteration 760, loss = 0.03955977\n",
      "Iteration 761, loss = 0.03942980\n",
      "Iteration 762, loss = 0.03930206\n",
      "Iteration 763, loss = 0.03917537\n",
      "Iteration 764, loss = 0.03904889\n",
      "Iteration 765, loss = 0.03891898\n",
      "Iteration 766, loss = 0.03878772\n",
      "Iteration 767, loss = 0.03866699\n",
      "Iteration 768, loss = 0.03854212\n",
      "Iteration 769, loss = 0.03841300\n",
      "Iteration 770, loss = 0.03829217\n",
      "Iteration 771, loss = 0.03817002\n",
      "Iteration 772, loss = 0.03804645\n",
      "Iteration 773, loss = 0.03792347\n",
      "Iteration 774, loss = 0.03779880\n",
      "Iteration 775, loss = 0.03767756\n",
      "Iteration 776, loss = 0.03755805\n",
      "Iteration 777, loss = 0.03743816\n",
      "Iteration 778, loss = 0.03731909\n",
      "Iteration 779, loss = 0.03719766\n",
      "Iteration 780, loss = 0.03708031\n",
      "Iteration 781, loss = 0.03696393\n",
      "Iteration 782, loss = 0.03684922\n",
      "Iteration 783, loss = 0.03672954\n",
      "Iteration 784, loss = 0.03660959\n",
      "Iteration 785, loss = 0.03649678\n",
      "Iteration 786, loss = 0.03638275\n",
      "Iteration 787, loss = 0.03626510\n",
      "Iteration 788, loss = 0.03615040\n",
      "Iteration 789, loss = 0.03603869\n",
      "Iteration 790, loss = 0.03592482\n",
      "Iteration 791, loss = 0.03580983\n",
      "Iteration 792, loss = 0.03570066\n",
      "Iteration 793, loss = 0.03559190\n",
      "Iteration 794, loss = 0.03547880\n",
      "Iteration 795, loss = 0.03536496\n",
      "Iteration 796, loss = 0.03525837\n",
      "Iteration 797, loss = 0.03514964\n",
      "Iteration 798, loss = 0.03504135\n",
      "Iteration 799, loss = 0.03493012\n",
      "Iteration 800, loss = 0.03481833\n",
      "Iteration 801, loss = 0.03471201\n",
      "Iteration 802, loss = 0.03460632\n",
      "Iteration 803, loss = 0.03450324\n",
      "Iteration 804, loss = 0.03439741\n",
      "Iteration 805, loss = 0.03428774\n",
      "Iteration 806, loss = 0.03418392\n",
      "Iteration 807, loss = 0.03407886\n",
      "Iteration 808, loss = 0.03397088\n",
      "Iteration 809, loss = 0.03386380\n",
      "Iteration 810, loss = 0.03376428\n",
      "Iteration 811, loss = 0.03366186\n",
      "Iteration 812, loss = 0.03355927\n",
      "Iteration 813, loss = 0.03345609\n",
      "Iteration 814, loss = 0.03335173\n",
      "Iteration 815, loss = 0.03325120\n",
      "Iteration 816, loss = 0.03315246\n",
      "Iteration 817, loss = 0.03305091\n",
      "Iteration 818, loss = 0.03295176\n",
      "Iteration 819, loss = 0.03284906\n",
      "Iteration 820, loss = 0.03275109\n",
      "Iteration 821, loss = 0.03265538\n",
      "Iteration 822, loss = 0.03255588\n",
      "Iteration 823, loss = 0.03245668\n",
      "Iteration 824, loss = 0.03235620\n",
      "Iteration 825, loss = 0.03226046\n",
      "Iteration 826, loss = 0.03216519\n",
      "Iteration 827, loss = 0.03206947\n",
      "Iteration 828, loss = 0.03197246\n",
      "Iteration 829, loss = 0.03187744\n",
      "Iteration 830, loss = 0.03178256\n",
      "Iteration 831, loss = 0.03168834\n",
      "Iteration 832, loss = 0.03159544\n",
      "Iteration 833, loss = 0.03150163\n",
      "Iteration 834, loss = 0.03140901\n",
      "Iteration 835, loss = 0.03131546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp = MLPClassifier( hidden_layer_sizes=(100,), max_iter=1000 ,verbose=True)\n",
    "model_mlp.fit( x_data, y_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248574a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict( x_data ) # 4x2 2x100   4x100  100x1 = 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f71314a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.score( x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0a98e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( model_mlp.coefs_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bfab716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.coefs_[0].shape # 2x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc7800be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.coefs_[1].shape #100x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe869af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.intercepts_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83544e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "294d1860",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70217362\n",
      "Iteration 2, loss = 0.69537637\n",
      "Iteration 3, loss = 0.68906465\n",
      "Iteration 4, loss = 0.68309226\n",
      "Iteration 5, loss = 0.67779556\n",
      "Iteration 6, loss = 0.67259985\n",
      "Iteration 7, loss = 0.66797556\n",
      "Iteration 8, loss = 0.66417529\n",
      "Iteration 9, loss = 0.66054199\n",
      "Iteration 10, loss = 0.65693636\n",
      "Iteration 11, loss = 0.65332473\n",
      "Iteration 12, loss = 0.64990460\n",
      "Iteration 13, loss = 0.64694893\n",
      "Iteration 14, loss = 0.64403679\n",
      "Iteration 15, loss = 0.64110794\n",
      "Iteration 16, loss = 0.63814645\n",
      "Iteration 17, loss = 0.63522297\n",
      "Iteration 18, loss = 0.63235035\n",
      "Iteration 19, loss = 0.62936062\n",
      "Iteration 20, loss = 0.62633499\n",
      "Iteration 21, loss = 0.62340082\n",
      "Iteration 22, loss = 0.62041995\n",
      "Iteration 23, loss = 0.61738668\n",
      "Iteration 24, loss = 0.61417825\n",
      "Iteration 25, loss = 0.61097891\n",
      "Iteration 26, loss = 0.60778057\n",
      "Iteration 27, loss = 0.60447438\n",
      "Iteration 28, loss = 0.60121051\n",
      "Iteration 29, loss = 0.59792542\n",
      "Iteration 30, loss = 0.59468718\n",
      "Iteration 31, loss = 0.59165453\n",
      "Iteration 32, loss = 0.58835353\n",
      "Iteration 33, loss = 0.58501517\n",
      "Iteration 34, loss = 0.58153481\n",
      "Iteration 35, loss = 0.57798983\n",
      "Iteration 36, loss = 0.57448460\n",
      "Iteration 37, loss = 0.57072310\n",
      "Iteration 38, loss = 0.56692858\n",
      "Iteration 39, loss = 0.56320679\n",
      "Iteration 40, loss = 0.55957463\n",
      "Iteration 41, loss = 0.55572631\n",
      "Iteration 42, loss = 0.55174630\n",
      "Iteration 43, loss = 0.54769566\n",
      "Iteration 44, loss = 0.54372424\n",
      "Iteration 45, loss = 0.53981921\n",
      "Iteration 46, loss = 0.53571504\n",
      "Iteration 47, loss = 0.53137501\n",
      "Iteration 48, loss = 0.52697842\n",
      "Iteration 49, loss = 0.52253694\n",
      "Iteration 50, loss = 0.51820801\n",
      "Iteration 51, loss = 0.51367106\n",
      "Iteration 52, loss = 0.50903890\n",
      "Iteration 53, loss = 0.50432167\n",
      "Iteration 54, loss = 0.49934209\n",
      "Iteration 55, loss = 0.49432650\n",
      "Iteration 56, loss = 0.48927344\n",
      "Iteration 57, loss = 0.48403099\n",
      "Iteration 58, loss = 0.47862956\n",
      "Iteration 59, loss = 0.47352451\n",
      "Iteration 60, loss = 0.46859263\n",
      "Iteration 61, loss = 0.46327614\n",
      "Iteration 62, loss = 0.45787604\n",
      "Iteration 63, loss = 0.45267449\n",
      "Iteration 64, loss = 0.44726625\n",
      "Iteration 65, loss = 0.44166331\n",
      "Iteration 66, loss = 0.43617603\n",
      "Iteration 67, loss = 0.43055366\n",
      "Iteration 68, loss = 0.42497949\n",
      "Iteration 69, loss = 0.41928932\n",
      "Iteration 70, loss = 0.41354090\n",
      "Iteration 71, loss = 0.40776537\n",
      "Iteration 72, loss = 0.40197673\n",
      "Iteration 73, loss = 0.39614634\n",
      "Iteration 74, loss = 0.39010519\n",
      "Iteration 75, loss = 0.38421959\n",
      "Iteration 76, loss = 0.37848244\n",
      "Iteration 77, loss = 0.37248220\n",
      "Iteration 78, loss = 0.36641688\n",
      "Iteration 79, loss = 0.36030781\n",
      "Iteration 80, loss = 0.35430859\n",
      "Iteration 81, loss = 0.34826935\n",
      "Iteration 82, loss = 0.34215192\n",
      "Iteration 83, loss = 0.33607263\n",
      "Iteration 84, loss = 0.33001057\n",
      "Iteration 85, loss = 0.32387957\n",
      "Iteration 86, loss = 0.31795595\n",
      "Iteration 87, loss = 0.31191287\n",
      "Iteration 88, loss = 0.30582829\n",
      "Iteration 89, loss = 0.29973633\n",
      "Iteration 90, loss = 0.29385084\n",
      "Iteration 91, loss = 0.28785584\n",
      "Iteration 92, loss = 0.28202059\n",
      "Iteration 93, loss = 0.27620092\n",
      "Iteration 94, loss = 0.27023632\n",
      "Iteration 95, loss = 0.26438044\n",
      "Iteration 96, loss = 0.25868002\n",
      "Iteration 97, loss = 0.25294179\n",
      "Iteration 98, loss = 0.24724743\n",
      "Iteration 99, loss = 0.24166801\n",
      "Iteration 100, loss = 0.23608293\n",
      "Iteration 101, loss = 0.23067142\n",
      "Iteration 102, loss = 0.22539707\n",
      "Iteration 103, loss = 0.22010102\n",
      "Iteration 104, loss = 0.21495538\n",
      "Iteration 105, loss = 0.20979128\n",
      "Iteration 106, loss = 0.20469600\n",
      "Iteration 107, loss = 0.19972960\n",
      "Iteration 108, loss = 0.19479384\n",
      "Iteration 109, loss = 0.18998025\n",
      "Iteration 110, loss = 0.18528833\n",
      "Iteration 111, loss = 0.18068065\n",
      "Iteration 112, loss = 0.17613309\n",
      "Iteration 113, loss = 0.17165681\n",
      "Iteration 114, loss = 0.16734497\n",
      "Iteration 115, loss = 0.16310180\n",
      "Iteration 116, loss = 0.15893988\n",
      "Iteration 117, loss = 0.15482418\n",
      "Iteration 118, loss = 0.15084599\n",
      "Iteration 119, loss = 0.14693439\n",
      "Iteration 120, loss = 0.14316329\n",
      "Iteration 121, loss = 0.13950276\n",
      "Iteration 122, loss = 0.13586853\n",
      "Iteration 123, loss = 0.13226934\n",
      "Iteration 124, loss = 0.12880540\n",
      "Iteration 125, loss = 0.12544820\n",
      "Iteration 126, loss = 0.12219855\n",
      "Iteration 127, loss = 0.11902994\n",
      "Iteration 128, loss = 0.11591681\n",
      "Iteration 129, loss = 0.11291111\n",
      "Iteration 130, loss = 0.10997713\n",
      "Iteration 131, loss = 0.10710449\n",
      "Iteration 132, loss = 0.10432519\n",
      "Iteration 133, loss = 0.10164617\n",
      "Iteration 134, loss = 0.09904317\n",
      "Iteration 135, loss = 0.09651247\n",
      "Iteration 136, loss = 0.09403779\n",
      "Iteration 137, loss = 0.09163855\n",
      "Iteration 138, loss = 0.08932771\n",
      "Iteration 139, loss = 0.08706496\n",
      "Iteration 140, loss = 0.08489533\n",
      "Iteration 141, loss = 0.08277976\n",
      "Iteration 142, loss = 0.08072874\n",
      "Iteration 143, loss = 0.07871726\n",
      "Iteration 144, loss = 0.07679376\n",
      "Iteration 145, loss = 0.07492855\n",
      "Iteration 146, loss = 0.07311680\n",
      "Iteration 147, loss = 0.07135686\n",
      "Iteration 148, loss = 0.06968551\n",
      "Iteration 149, loss = 0.06804711\n",
      "Iteration 150, loss = 0.06644610\n",
      "Iteration 151, loss = 0.06490681\n",
      "Iteration 152, loss = 0.06340843\n",
      "Iteration 153, loss = 0.06193167\n",
      "Iteration 154, loss = 0.06049748\n",
      "Iteration 155, loss = 0.05915240\n",
      "Iteration 156, loss = 0.05784428\n",
      "Iteration 157, loss = 0.05655888\n",
      "Iteration 158, loss = 0.05531968\n",
      "Iteration 159, loss = 0.05412024\n",
      "Iteration 160, loss = 0.05294331\n",
      "Iteration 161, loss = 0.05180516\n",
      "Iteration 162, loss = 0.05071077\n",
      "Iteration 163, loss = 0.04964966\n",
      "Iteration 164, loss = 0.04861814\n",
      "Iteration 165, loss = 0.04762734\n",
      "Iteration 166, loss = 0.04666451\n",
      "Iteration 167, loss = 0.04571916\n",
      "Iteration 168, loss = 0.04480200\n",
      "Iteration 169, loss = 0.04389444\n",
      "Iteration 170, loss = 0.04301858\n",
      "Iteration 171, loss = 0.04218320\n",
      "Iteration 172, loss = 0.04136369\n",
      "Iteration 173, loss = 0.04057391\n",
      "Iteration 174, loss = 0.03980701\n",
      "Iteration 175, loss = 0.03906277\n",
      "Iteration 176, loss = 0.03832894\n",
      "Iteration 177, loss = 0.03762561\n",
      "Iteration 178, loss = 0.03693597\n",
      "Iteration 179, loss = 0.03626022\n",
      "Iteration 180, loss = 0.03560057\n",
      "Iteration 181, loss = 0.03496869\n",
      "Iteration 182, loss = 0.03434977\n",
      "Iteration 183, loss = 0.03374407\n",
      "Iteration 184, loss = 0.03315585\n",
      "Iteration 185, loss = 0.03258643\n",
      "Iteration 186, loss = 0.03203518\n",
      "Iteration 187, loss = 0.03149353\n",
      "Iteration 188, loss = 0.03096552\n",
      "Iteration 189, loss = 0.03044718\n",
      "Iteration 190, loss = 0.02995044\n",
      "Iteration 191, loss = 0.02946367\n",
      "Iteration 192, loss = 0.02898533\n",
      "Iteration 193, loss = 0.02852419\n",
      "Iteration 194, loss = 0.02807088\n",
      "Iteration 195, loss = 0.02762628\n",
      "Iteration 196, loss = 0.02720113\n",
      "Iteration 197, loss = 0.02678542\n",
      "Iteration 198, loss = 0.02637636\n",
      "Iteration 199, loss = 0.02597540\n",
      "Iteration 200, loss = 0.02558388\n",
      "Iteration 201, loss = 0.02519879\n",
      "Iteration 202, loss = 0.02482101\n",
      "Iteration 203, loss = 0.02446007\n",
      "Iteration 204, loss = 0.02409520\n",
      "Iteration 205, loss = 0.02373955\n",
      "Iteration 206, loss = 0.02339319\n",
      "Iteration 207, loss = 0.02305424\n",
      "Iteration 208, loss = 0.02271728\n",
      "Iteration 209, loss = 0.02239245\n",
      "Iteration 210, loss = 0.02207392\n",
      "Iteration 211, loss = 0.02175870\n",
      "Iteration 212, loss = 0.02145188\n",
      "Iteration 213, loss = 0.02115043\n",
      "Iteration 214, loss = 0.02085140\n",
      "Iteration 215, loss = 0.02055944\n",
      "Iteration 216, loss = 0.02027249\n",
      "Iteration 217, loss = 0.01999308\n",
      "Iteration 218, loss = 0.01971826\n",
      "Iteration 219, loss = 0.01945008\n",
      "Iteration 220, loss = 0.01918291\n",
      "Iteration 221, loss = 0.01891833\n",
      "Iteration 222, loss = 0.01866369\n",
      "Iteration 223, loss = 0.01841443\n",
      "Iteration 224, loss = 0.01816748\n",
      "Iteration 225, loss = 0.01792356\n",
      "Iteration 226, loss = 0.01768739\n",
      "Iteration 227, loss = 0.01745413\n",
      "Iteration 228, loss = 0.01722423\n",
      "Iteration 229, loss = 0.01700023\n",
      "Iteration 230, loss = 0.01677798\n",
      "Iteration 231, loss = 0.01656110\n",
      "Iteration 232, loss = 0.01634919\n",
      "Iteration 233, loss = 0.01613983\n",
      "Iteration 234, loss = 0.01593634\n",
      "Iteration 235, loss = 0.01573207\n",
      "Iteration 236, loss = 0.01553294\n",
      "Iteration 237, loss = 0.01533862\n",
      "Iteration 238, loss = 0.01514588\n",
      "Iteration 239, loss = 0.01495841\n",
      "Iteration 240, loss = 0.01477573\n",
      "Iteration 241, loss = 0.01459496\n",
      "Iteration 242, loss = 0.01441576\n",
      "Iteration 243, loss = 0.01424172\n",
      "Iteration 244, loss = 0.01406892\n",
      "Iteration 245, loss = 0.01389957\n",
      "Iteration 246, loss = 0.01373414\n",
      "Iteration 247, loss = 0.01357238\n",
      "Iteration 248, loss = 0.01341306\n",
      "Iteration 249, loss = 0.01325561\n",
      "Iteration 250, loss = 0.01310170\n",
      "Iteration 251, loss = 0.01294802\n",
      "Iteration 252, loss = 0.01279707\n",
      "Iteration 253, loss = 0.01265164\n",
      "Iteration 254, loss = 0.01250936\n",
      "Iteration 255, loss = 0.01236595\n",
      "Iteration 256, loss = 0.01222490\n",
      "Iteration 257, loss = 0.01208898\n",
      "Iteration 258, loss = 0.01195593\n",
      "Iteration 259, loss = 0.01182360\n",
      "Iteration 260, loss = 0.01169251\n",
      "Iteration 261, loss = 0.01156554\n",
      "Iteration 262, loss = 0.01144035\n",
      "Iteration 263, loss = 0.01131553\n",
      "Iteration 264, loss = 0.01119512\n",
      "Iteration 265, loss = 0.01107516\n",
      "Iteration 266, loss = 0.01095762\n",
      "Iteration 267, loss = 0.01084113\n",
      "Iteration 268, loss = 0.01072664\n",
      "Iteration 269, loss = 0.01061398\n",
      "Iteration 270, loss = 0.01050265\n",
      "Iteration 271, loss = 0.01039619\n",
      "Iteration 272, loss = 0.01028586\n",
      "Iteration 273, loss = 0.01017898\n",
      "Iteration 274, loss = 0.01007497\n",
      "Iteration 275, loss = 0.00997187\n",
      "Iteration 276, loss = 0.00987063\n",
      "Iteration 277, loss = 0.00977002\n",
      "Iteration 278, loss = 0.00967036\n",
      "Iteration 279, loss = 0.00957183\n",
      "Iteration 280, loss = 0.00947490\n",
      "Iteration 281, loss = 0.00937961\n",
      "Iteration 282, loss = 0.00928631\n",
      "Iteration 283, loss = 0.00919296\n",
      "Iteration 284, loss = 0.00910078\n",
      "Iteration 285, loss = 0.00901105\n",
      "Iteration 286, loss = 0.00892248\n",
      "Iteration 287, loss = 0.00883421\n",
      "Iteration 288, loss = 0.00874828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1 = MLPClassifier( hidden_layer_sizes=(100,50), max_iter=1000 ,verbose=True)\n",
    "model_mlp1.fit( x_data, y_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dedf7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.predict( x_data) #4x2 2x100 = 4x100 100x50 = 4x50 50x1 = 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02af1d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( model_mlp1.coefs_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242c050b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "420fb843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62c15b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[2].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bfecb93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71876693\n",
      "Iteration 2, loss = 0.71346115\n",
      "Iteration 3, loss = 0.70955802\n",
      "Iteration 4, loss = 0.70639993\n",
      "Iteration 5, loss = 0.70303083\n",
      "Iteration 6, loss = 0.69976576\n",
      "Iteration 7, loss = 0.69675719\n",
      "Iteration 8, loss = 0.69366913\n",
      "Iteration 9, loss = 0.69051938\n",
      "Iteration 10, loss = 0.68741031\n",
      "Iteration 11, loss = 0.68438339\n",
      "Iteration 12, loss = 0.68211264\n",
      "Iteration 13, loss = 0.67970727\n",
      "Iteration 14, loss = 0.67717782\n",
      "Iteration 15, loss = 0.67464454\n",
      "Iteration 16, loss = 0.67225888\n",
      "Iteration 17, loss = 0.66974165\n",
      "Iteration 18, loss = 0.66711494\n",
      "Iteration 19, loss = 0.66444714\n",
      "Iteration 20, loss = 0.66176164\n",
      "Iteration 21, loss = 0.65926432\n",
      "Iteration 22, loss = 0.65677943\n",
      "Iteration 23, loss = 0.65438387\n",
      "Iteration 24, loss = 0.65190198\n",
      "Iteration 25, loss = 0.64929731\n",
      "Iteration 26, loss = 0.64653555\n",
      "Iteration 27, loss = 0.64384103\n",
      "Iteration 28, loss = 0.64108366\n",
      "Iteration 29, loss = 0.63838417\n",
      "Iteration 30, loss = 0.63560963\n",
      "Iteration 31, loss = 0.63278140\n",
      "Iteration 32, loss = 0.62981065\n",
      "Iteration 33, loss = 0.62678117\n",
      "Iteration 34, loss = 0.62368085\n",
      "Iteration 35, loss = 0.62060500\n",
      "Iteration 36, loss = 0.61757320\n",
      "Iteration 37, loss = 0.61442693\n",
      "Iteration 38, loss = 0.61124890\n",
      "Iteration 39, loss = 0.60815846\n",
      "Iteration 40, loss = 0.60502083\n",
      "Iteration 41, loss = 0.60173704\n",
      "Iteration 42, loss = 0.59836982\n",
      "Iteration 43, loss = 0.59490380\n",
      "Iteration 44, loss = 0.59137985\n",
      "Iteration 45, loss = 0.58772292\n",
      "Iteration 46, loss = 0.58410411\n",
      "Iteration 47, loss = 0.58021785\n",
      "Iteration 48, loss = 0.57634684\n",
      "Iteration 49, loss = 0.57235583\n",
      "Iteration 50, loss = 0.56839016\n",
      "Iteration 51, loss = 0.56419836\n",
      "Iteration 52, loss = 0.55981950\n",
      "Iteration 53, loss = 0.55538961\n",
      "Iteration 54, loss = 0.55075373\n",
      "Iteration 55, loss = 0.54617772\n",
      "Iteration 56, loss = 0.54132759\n",
      "Iteration 57, loss = 0.53639866\n",
      "Iteration 58, loss = 0.53161750\n",
      "Iteration 59, loss = 0.52657206\n",
      "Iteration 60, loss = 0.52146841\n",
      "Iteration 61, loss = 0.51626061\n",
      "Iteration 62, loss = 0.51091915\n",
      "Iteration 63, loss = 0.50538492\n",
      "Iteration 64, loss = 0.49957716\n",
      "Iteration 65, loss = 0.49400638\n",
      "Iteration 66, loss = 0.48819007\n",
      "Iteration 67, loss = 0.48215764\n",
      "Iteration 68, loss = 0.47604278\n",
      "Iteration 69, loss = 0.47013264\n",
      "Iteration 70, loss = 0.46374002\n",
      "Iteration 71, loss = 0.45743992\n",
      "Iteration 72, loss = 0.45103367\n",
      "Iteration 73, loss = 0.44439483\n",
      "Iteration 74, loss = 0.43816358\n",
      "Iteration 75, loss = 0.43139875\n",
      "Iteration 76, loss = 0.42462702\n",
      "Iteration 77, loss = 0.41779136\n",
      "Iteration 78, loss = 0.41069546\n",
      "Iteration 79, loss = 0.40367645\n",
      "Iteration 80, loss = 0.39662049\n",
      "Iteration 81, loss = 0.38948273\n",
      "Iteration 82, loss = 0.38240188\n",
      "Iteration 83, loss = 0.37500813\n",
      "Iteration 84, loss = 0.36756132\n",
      "Iteration 85, loss = 0.36015552\n",
      "Iteration 86, loss = 0.35286359\n",
      "Iteration 87, loss = 0.34542545\n",
      "Iteration 88, loss = 0.33769275\n",
      "Iteration 89, loss = 0.33034606\n",
      "Iteration 90, loss = 0.32287365\n",
      "Iteration 91, loss = 0.31536769\n",
      "Iteration 92, loss = 0.30806135\n",
      "Iteration 93, loss = 0.30058065\n",
      "Iteration 94, loss = 0.29311677\n",
      "Iteration 95, loss = 0.28562426\n",
      "Iteration 96, loss = 0.27827804\n",
      "Iteration 97, loss = 0.27107272\n",
      "Iteration 98, loss = 0.26381443\n",
      "Iteration 99, loss = 0.25649864\n",
      "Iteration 100, loss = 0.24938436\n",
      "Iteration 101, loss = 0.24248783\n",
      "Iteration 102, loss = 0.23567356\n",
      "Iteration 103, loss = 0.22890052\n",
      "Iteration 104, loss = 0.22209798\n",
      "Iteration 105, loss = 0.21545692\n",
      "Iteration 106, loss = 0.20896026\n",
      "Iteration 107, loss = 0.20252798\n",
      "Iteration 108, loss = 0.19621289\n",
      "Iteration 109, loss = 0.19006662\n",
      "Iteration 110, loss = 0.18402150\n",
      "Iteration 111, loss = 0.17813669\n",
      "Iteration 112, loss = 0.17228239\n",
      "Iteration 113, loss = 0.16651894\n",
      "Iteration 114, loss = 0.16094087\n",
      "Iteration 115, loss = 0.15554358\n",
      "Iteration 116, loss = 0.15024794\n",
      "Iteration 117, loss = 0.14511696\n",
      "Iteration 118, loss = 0.14018816\n",
      "Iteration 119, loss = 0.13536728\n",
      "Iteration 120, loss = 0.13075501\n",
      "Iteration 121, loss = 0.12614419\n",
      "Iteration 122, loss = 0.12176132\n",
      "Iteration 123, loss = 0.11751318\n",
      "Iteration 124, loss = 0.11337161\n",
      "Iteration 125, loss = 0.10933946\n",
      "Iteration 126, loss = 0.10545205\n",
      "Iteration 127, loss = 0.10172145\n",
      "Iteration 128, loss = 0.09813330\n",
      "Iteration 129, loss = 0.09467694\n",
      "Iteration 130, loss = 0.09134751\n",
      "Iteration 131, loss = 0.08813979\n",
      "Iteration 132, loss = 0.08506290\n",
      "Iteration 133, loss = 0.08208607\n",
      "Iteration 134, loss = 0.07920052\n",
      "Iteration 135, loss = 0.07650157\n",
      "Iteration 136, loss = 0.07383592\n",
      "Iteration 137, loss = 0.07131743\n",
      "Iteration 138, loss = 0.06885182\n",
      "Iteration 139, loss = 0.06649349\n",
      "Iteration 140, loss = 0.06427339\n",
      "Iteration 141, loss = 0.06216946\n",
      "Iteration 142, loss = 0.06012257\n",
      "Iteration 143, loss = 0.05808712\n",
      "Iteration 144, loss = 0.05622086\n",
      "Iteration 145, loss = 0.05440300\n",
      "Iteration 146, loss = 0.05264062\n",
      "Iteration 147, loss = 0.05095116\n",
      "Iteration 148, loss = 0.04936684\n",
      "Iteration 149, loss = 0.04782028\n",
      "Iteration 150, loss = 0.04634264\n",
      "Iteration 151, loss = 0.04493525\n",
      "Iteration 152, loss = 0.04357814\n",
      "Iteration 153, loss = 0.04227467\n",
      "Iteration 154, loss = 0.04104579\n",
      "Iteration 155, loss = 0.03986476\n",
      "Iteration 156, loss = 0.03871621\n",
      "Iteration 157, loss = 0.03761234\n",
      "Iteration 158, loss = 0.03655407\n",
      "Iteration 159, loss = 0.03554948\n",
      "Iteration 160, loss = 0.03458739\n",
      "Iteration 161, loss = 0.03364518\n",
      "Iteration 162, loss = 0.03274069\n",
      "Iteration 163, loss = 0.03187107\n",
      "Iteration 164, loss = 0.03104533\n",
      "Iteration 165, loss = 0.03025129\n",
      "Iteration 166, loss = 0.02947603\n",
      "Iteration 167, loss = 0.02874462\n",
      "Iteration 168, loss = 0.02802645\n",
      "Iteration 169, loss = 0.02734430\n",
      "Iteration 170, loss = 0.02668853\n",
      "Iteration 171, loss = 0.02604541\n",
      "Iteration 172, loss = 0.02542520\n",
      "Iteration 173, loss = 0.02484199\n",
      "Iteration 174, loss = 0.02426816\n",
      "Iteration 175, loss = 0.02371757\n",
      "Iteration 176, loss = 0.02318611\n",
      "Iteration 177, loss = 0.02267153\n",
      "Iteration 178, loss = 0.02217482\n",
      "Iteration 179, loss = 0.02170103\n",
      "Iteration 180, loss = 0.02124328\n",
      "Iteration 181, loss = 0.02079881\n",
      "Iteration 182, loss = 0.02036707\n",
      "Iteration 183, loss = 0.01995073\n",
      "Iteration 184, loss = 0.01954674\n",
      "Iteration 185, loss = 0.01915372\n",
      "Iteration 186, loss = 0.01877533\n",
      "Iteration 187, loss = 0.01840557\n",
      "Iteration 188, loss = 0.01805077\n",
      "Iteration 189, loss = 0.01770651\n",
      "Iteration 190, loss = 0.01737366\n",
      "Iteration 191, loss = 0.01705336\n",
      "Iteration 192, loss = 0.01674097\n",
      "Iteration 193, loss = 0.01643446\n",
      "Iteration 194, loss = 0.01613680\n",
      "Iteration 195, loss = 0.01584966\n",
      "Iteration 196, loss = 0.01556942\n",
      "Iteration 197, loss = 0.01530000\n",
      "Iteration 198, loss = 0.01503873\n",
      "Iteration 199, loss = 0.01478258\n",
      "Iteration 200, loss = 0.01453192\n",
      "Iteration 201, loss = 0.01428905\n",
      "Iteration 202, loss = 0.01405380\n",
      "Iteration 203, loss = 0.01382702\n",
      "Iteration 204, loss = 0.01360409\n",
      "Iteration 205, loss = 0.01338785\n",
      "Iteration 206, loss = 0.01317622\n",
      "Iteration 207, loss = 0.01297074\n",
      "Iteration 208, loss = 0.01277391\n",
      "Iteration 209, loss = 0.01257829\n",
      "Iteration 210, loss = 0.01238864\n",
      "Iteration 211, loss = 0.01220288\n",
      "Iteration 212, loss = 0.01202198\n",
      "Iteration 213, loss = 0.01184575\n",
      "Iteration 214, loss = 0.01167378\n",
      "Iteration 215, loss = 0.01150431\n",
      "Iteration 216, loss = 0.01134041\n",
      "Iteration 217, loss = 0.01118142\n",
      "Iteration 218, loss = 0.01102578\n",
      "Iteration 219, loss = 0.01087258\n",
      "Iteration 220, loss = 0.01072235\n",
      "Iteration 221, loss = 0.01057817\n",
      "Iteration 222, loss = 0.01043750\n",
      "Iteration 223, loss = 0.01029851\n",
      "Iteration 224, loss = 0.01016313\n",
      "Iteration 225, loss = 0.01002899\n",
      "Iteration 226, loss = 0.00989889\n",
      "Iteration 227, loss = 0.00977214\n",
      "Iteration 228, loss = 0.00964789\n",
      "Iteration 229, loss = 0.00952675\n",
      "Iteration 230, loss = 0.00940930\n",
      "Iteration 231, loss = 0.00929381\n",
      "Iteration 232, loss = 0.00917970\n",
      "Iteration 233, loss = 0.00906846\n",
      "Iteration 234, loss = 0.00895907\n",
      "Iteration 235, loss = 0.00885266\n",
      "Iteration 236, loss = 0.00874852\n",
      "Iteration 237, loss = 0.00864672\n",
      "Iteration 238, loss = 0.00854674\n",
      "Iteration 239, loss = 0.00844677\n",
      "Iteration 240, loss = 0.00835066\n",
      "Iteration 241, loss = 0.00825672\n",
      "Iteration 242, loss = 0.00816413\n",
      "Iteration 243, loss = 0.00807333\n",
      "Iteration 244, loss = 0.00798502\n",
      "Iteration 245, loss = 0.00789814\n",
      "Iteration 246, loss = 0.00781319\n",
      "Iteration 247, loss = 0.00772975\n",
      "Iteration 248, loss = 0.00764750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2 = MLPClassifier( hidden_layer_sizes=(64,32,16), \n",
    "                           max_iter=1000 ,verbose=True)\n",
    "model_mlp2.fit( x_data, y_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b6280d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( model_mlp2.coefs_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e506b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "566765f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77146fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d3dc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
